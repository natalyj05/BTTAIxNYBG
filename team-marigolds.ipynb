{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66199,"databundleVersionId":7522884,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# STEP 1: Pre-Processing Starter Notebook","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\n\n# Define directories\ntrain_dir = '/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-train/BTTAIxNYBG-train'\ntest_dir = '/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-test/BTTAIxNYBG-test'\nvalidation_dir = '/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-validation/BTTAIxNYBG-validation'\n\n# Load datasets\ntrain_df = pd.read_csv('/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-train.csv')\ntest_df = pd.read_csv('/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-test.csv')\nvalidate_df = pd.read_csv('/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-validation.csv')\n\n# Data augmentation configuration for training\ntrain_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    rescale=1./255\n)\n\n# Note: No augmentation for validation and test data, only rescaling\nvalidation_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# Convert dataframe to a format suitable for the model training\ndef df_to_dataset(dataframe, datagen, directory, batch_size=32):\n    return datagen.flow_from_dataframe(\n        dataframe=dataframe,\n        directory=directory,\n        x_col='imageFile',  # Column in dataframe that contains the filenames\n        y_col='classLabel',  # Column in dataframe that contains the class/label\n        target_size=(224, 224),\n        batch_size=batch_size,\n        class_mode='categorical'  # Change this if not a multiclass classification\n    )\n\n# Create datasets for training, validation, and testing\ntrain_dataset = df_to_dataset(train_df, train_datagen, train_dir)\nvalidation_dataset = df_to_dataset(validate_df, validation_datagen, validation_dir)\n\n# This setup is now ready for training with model.fit using the train_dataset and validation_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import MobileNetV2\n\n# Load the MobileNetV2 model, pretrained on ImageNet, without the top layer\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Create the model\nmodel = Sequential([\n    base_model,\n    # Convert features to vectors\n    tf.keras.layers.GlobalAveragePooling2D(),\n    # Add a dense layer for classification\n    Dense(1024, activation='relu'),\n    # Final layer with softmax activation for multi-class classification\n    Dense(10, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# Callbacks\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True),\n    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n]\n\n# Train the model\nhistory = model.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    epochs=1,\n    callbacks=callbacks\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# STEP 5: Evaluate the Model","metadata":{}},{"cell_type":"code","source":"validation_loss, validation_accuracy = model.evaluate(validation_dataset)\nprint(f'Validation Loss: {validation_loss}')\nprint(f'Validation Accuracy: {validation_accuracy}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# STEP 6: Make Predictions on Test Set","metadata":{}},{"cell_type":"code","source":"test_dataset = test_datagen.flow_from_dataframe(\n    dataframe=test_df,\n    directory=test_dir,\n    x_col='imageFile',  # Make sure this column name matches your test_df column name for filenames\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode=None,  # No labels\n    shuffle=False\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(test_dataset)\npredicted_class_indices = np.argmax(predictions, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# STEP 7: Prepare Submission File","metadata":{}},{"cell_type":"code","source":"labels = (train_dataset.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredicted_class_ids = [labels[v] for v in predicted_class_indices]\n\nsubmission_df = pd.DataFrame({'uniqueID': test_df['uniqueID'], 'classID': predicted_class_indices})\nsubmission_df.to_csv('submission.csv', index=False)\n","metadata":{},"execution_count":null,"outputs":[]}]}
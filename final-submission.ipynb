{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66199,"databundleVersionId":7522884,"sourceType":"competition"}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# Define directories where the train, test, and validation images are stored\ntrain_dir = '/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-train/BTTAIxNYBG-train'\ntest_dir = '/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-test/BTTAIxNYBG-test'\nvalidation_dir = '/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-validation/BTTAIxNYBG-validation'\n\n# Load dataframes containing metadata such as image filenames and class labels\ntrain_df = pd.read_csv('/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-train.csv')\ntest_df = pd.read_csv('/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-test.csv')\nvalidate_df = pd.read_csv('/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-validation.csv')\n\n# Data augmentation configuration for the training set to improve model generalization\ntrain_datagen = ImageDataGenerator(\n    rotation_range=40,  # Degrees of random rotations\n    width_shift_range=0.2,  # Fraction of total width, for horizontal shift\n    height_shift_range=0.2,  # Fraction of total height, for vertical shift\n    shear_range=0.2,  # Shear Intensity (Shear angle in counter-clockwise direction)\n    zoom_range=[0.8, 1.2],  # Range for random zoom. Now allows for zoom in and out\n    horizontal_flip=True,  # Randomly flip inputs horizontally\n    fill_mode='nearest',  # Strategy to fill in newly created pixels\n    brightness_range=[0.5, 1.5],  # Randomly alter the brightness of images\n    channel_shift_range=50.0,  # Range for random channel shifts\n    rescale=1./255  # Rescaling factor for normalizing pixel values\n)\n\n# Data generators for validation and test sets only rescale the images\nvalidation_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# Converts dataframe to a data generator (suitable for model training)\ndef df_to_dataset(dataframe, datagen, directory, batch_size=32):\n    return datagen.flow_from_dataframe(\n        dataframe=dataframe,\n        directory=directory,\n        x_col='imageFile',  # Column in dataframe that contains the filenames\n        y_col='classLabel',  # Column in dataframe that contains the class/label\n        target_size=(224, 224),\n        batch_size=batch_size,\n        class_mode='categorical'  # Multiclass classification\n    )\n\n# Create datasets for training and validation\ntrain_dataset = df_to_dataset(train_df, train_datagen, train_dir)\nvalidation_dataset = df_to_dataset(validate_df, validation_datagen, validation_dir)\n\n# Load MobileNetV2 pretrained on ImageNet and freeze the first 'fine_tune_at' layers\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\nfine_tune_at = 100\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable = False\n\n# Assemble the full model including new top layers\nmodel = Sequential([\n    base_model,\n    # Convert features to vectors\n    tf.keras.layers.GlobalAveragePooling2D(),\n    # Add a dense layer for classification\n    Dense(1024, activation='relu'),\n    # Final layer with softmax activation for multi-class classification\n    Dense(10, activation='softmax')\n])\n\n# Set up learning rate schedule and compile model\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=1e-4,\n    decay_steps=10000,\n    decay_rate=0.9\n)\nmodel.compile(optimizer=Adam(learning_rate=lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\n# Train the model on a subset of the training data for quick testing\ntrain_subset = train_df.sample(frac=0.7, random_state=42)\ntrain_subset_dataset = df_to_dataset(train_subset, train_datagen, train_dir)\n\n# Callbacks for early stopping and model checkpointing\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True),\n    ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n]\n\n# Train the model\nhistory = model.fit(\n    train_subset_dataset,  # Use the subset of data\n    validation_data=validation_dataset,\n    epochs=3,  # Initially train for fewer epochs for debugging\n    callbacks=callbacks\n)\n\n# Continue training on the entire dataset\nhistory = model.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    epochs=4, \n    callbacks=callbacks\n)\n\n# Evaluate the model on the validation set\nvalidation_loss, validation_accuracy = model.evaluate(validation_dataset)\nprint(f'Validation Loss: {validation_loss}')\nprint(f'Validation Accuracy: {validation_accuracy}')\n\n# Generate predictions on the test set\ntest_dataset = test_datagen.flow_from_dataframe(\n    dataframe=test_df,\n    directory=test_dir,\n    x_col='imageFile',  # Make sure column name matches test_df column name for filenames\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode=None,  # No labels\n    shuffle=False\n)\npredictions = model.predict(test_dataset)\npredicted_class_indices = np.argmax(predictions, axis=1)\n\n# Save predictions to a CSV file for submission\nsubmission_df = pd.DataFrame({'uniqueID': test_df['uniqueID'], 'classID': predicted_class_indices})\nsubmission_df.to_csv('/kaggle/working/submission.csv', index=False)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-10T00:57:15.628039Z","iopub.execute_input":"2024-06-10T00:57:15.628698Z","iopub.status.idle":"2024-06-10T04:36:36.123938Z","shell.execute_reply.started":"2024-06-10T00:57:15.628666Z","shell.execute_reply":"2024-06-10T04:36:36.122985Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found 81946 validated image filenames belonging to 10 classes.\nFound 10244 validated image filenames belonging to 10 classes.\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nFound 57362 validated image filenames belonging to 10 classes.\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m   1/1793\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:37:44\u001b[0m 33s/step - accuracy: 0.0938 - loss: 2.5207","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1717981261.157718      82 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1793/1793\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1671s\u001b[0m 914ms/step - accuracy: 0.8422 - loss: 0.4764 - val_accuracy: 0.8627 - val_loss: 0.5227\nEpoch 2/3\n\u001b[1m1793/1793\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1272s\u001b[0m 707ms/step - accuracy: 0.9306 - loss: 0.1963 - val_accuracy: 0.9379 - val_loss: 0.1863\nEpoch 3/3\n\u001b[1m1793/1793\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1265s\u001b[0m 703ms/step - accuracy: 0.9468 - loss: 0.1560 - val_accuracy: 0.9489 - val_loss: 0.1636\nRestoring model weights from the end of the best epoch: 3.\nEpoch 1/4\n\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2149s\u001b[0m 837ms/step - accuracy: 0.9491 - loss: 0.1421 - val_accuracy: 0.9398 - val_loss: 0.1980\nEpoch 2/4\n\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2186s\u001b[0m 851ms/step - accuracy: 0.9549 - loss: 0.1260 - val_accuracy: 0.9619 - val_loss: 0.1192\nEpoch 3/4\n\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1962s\u001b[0m 764ms/step - accuracy: 0.9598 - loss: 0.1144 - val_accuracy: 0.9625 - val_loss: 0.1113\nEpoch 4/4\n\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1902s\u001b[0m 741ms/step - accuracy: 0.9627 - loss: 0.1044 - val_accuracy: 0.9491 - val_loss: 0.1594\nRestoring model weights from the end of the best epoch: 3.\n\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 233ms/step - accuracy: 0.9664 - loss: 0.1020\nValidation Loss: 0.11152070015668869\nValidation Accuracy: 0.9625146389007568\nFound 30690 validated image filenames.\n\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 425ms/step\n","output_type":"stream"}]}]}